---
layout: page
title: ""
---

Resources below are for me as well as visitors of this page. These include data science, machine/deep learning related sources that I have visited before and that I found very instructive.  

## Books 
[Deep Learning](https://www.deeplearningbook.org/) (Ian Goodfellow, Yoshua Bengio, Aaron Courville)  <br> 
A beginner-intermediate level source for understanding what deep learning is, and for having an intuitive grasp of the material.  

[Deep Learning: Foundations and Concepts](https://www.bishopbook.com/) (Christopher M. Bishop, Hugh Bishop)  <br> 
A more recent treatise of deep learning concepts.   

[Reinforcement Learning: An Introduction](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=dp_ob_title_bk) (Andrew Barto, Richard S. Sutton) <br>
As it is clear from its name, this is a fantastic introduction to reinforcement learning. 

[Introduction to Linear Algebra](https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775) (Gilbert Strang) <br>
It is imperative to know fundamentals of Linear Algebra to do machine learning research, and this is a fantastic source. 

[Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969) (Chip Huyen) <br>
This is a book for the people working on deployment part. However, it is still a good source for researchers to appreciate the difficulty of maintaining these models for real customers.

[Mathematics for Machine Learning](https://mml-book.github.io/) (Marc Peter Deisenroth et. al.) <br>
Not sure if it is a good book for beginners, but a great one as a refresher. 


## Online Lectures
[Deep Generative Models](https://kuleshov-group.github.io/dgm-website/) (Volodymyr Kuleshov) <br>
The best resource out there for understanding generative models (as of now). The instructor makes a fabulous job in connecting different developments in generative modeling which makes it very easy to understand and follow. 

[Linear Algebra](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/video_galleries/video-lectures/) (Gilbert Strang) <br>
Another perfect source from Prof. Strang. These video lectures are old but the contents are still fresh for machine learning researchers. 

[Reinforcement Learning Course](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ) (David Silver) <br>
This was the source that I learned reinforcement learning from. Content might be a bit outdated but wonderful for getting the fundamentals. 

[Stanford Machine Learning Course](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) (Andrew Ng) <br>
This course is perfect for conventional machine learning basics. 

[Stanford CS231n - 2017 Video Lecture Series](https://www.youtube.com/playlist?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk) <br>
Great source for understanding the basics of deep learning.

[Applied Machine Learning - Cornell](https://kuleshov-group.github.io/aml-website/) (Volodymyr Kuleshov) <bv>
I used this one as a refresher. Very explanatory for the basics of conventional ML algorithms. 

[Statistical Tests](https://www.youtube.com/watch?v=Ym1iH8-GQOE&t=7916s&ab_channel=DATAtab) (DATATab) <br>
Very good source for understanding statistical tests.


## Websites/Videos
[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (Christopher Olah)
[Cool Post on PyTorch Debugging](https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/?t=1) (Elana P. Simon)
[On t-SNE](https://distill.pub/2016/misread-tsne/) (Distill)

## Papers
* LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." nature 521.7553 (2015): 436-444. 
* Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017). **Transformer Paper** 
* Mnih, Volodymyr, et al. "Playing atari with deep reinforcement learning." arXiv preprint arXiv:1312.5602 (2013). **DQN Paper** 
* Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015). **DDPG Paper**
* Haarnoja, Tuomas, et al. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." International conference on machine learning. PMLR, 2018. **SAC Paper**
* Fujimoto, Scott, Herke Hoof, and David Meger. "Addressing function approximation error in actor-critic methods." International conference on machine learning. PMLR, 2018. **TD3 Paper**
* Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. "U-net: Convolutional networks for biomedical image segmentation." Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer International Publishing, 2015. **U-Net Paper**
* Ploetz, Thomas. "Applying machine learning for sensor data analysis in interactive systems: Common pitfalls of pragmatic use and ways to avoid them." ACM Computing Surveys (CSUR) 54.6 (2021): 1-25.
* Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. "Representation learning with contrastive predictive coding." arXiv preprint arXiv:1807.03748 (2018). **InfoNCE Loss**
* Doersch, Carl. "Tutorial on variational autoencoders." arXiv preprint arXiv:1606.05908 (2016). **VAE**
* Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018). **BERT**
* Liang, Paul Pu, Amir Zadeh, and Louis-Philippe Morency. "Foundations & Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions." ACM Computing Surveys (2023). **Multi-modality**
* Fairclough, Stephen H. "Fundamentals of physiological computing." Interacting with computers 21.1-2 (2009): 133-145. **Psychophysiological Computing**
* Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. "Isolation forest." 2008 eighth ieee international conference on data mining. IEEE, 2008. **Isolation Forest**
* Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural computation 9.8 (1997): 1735-1780. **LSTM**
* Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems 25 (2012).  **AlexNet**
* Schölkopf, Bernhard, et al. "Toward causal representation learning." Proceedings of the IEEE 109.5 (2021): 612-634. **Causal Learning**
* Girshick, Ross, et al. "Rich feature hierarchies for accurate object detection and semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. **R-CNN**
* Girshick, Ross. "Fast r-cnn." Proceedings of the IEEE international conference on computer vision. 2015. **Fast R-CNN**
* Ren, Shaoqing, et al. "Faster r-cnn: Towards real-time object detection with region proposal networks." Advances in neural information processing systems 28 (2015). **Faster R-CNN**
* Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. **FCN**
* Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. **YOLO**
* Burges, Christopher JC. "A tutorial on support vector machines for pattern recognition." Data mining and knowledge discovery 2.2 (1998): 121-167. **SVM**
* Breiman, Leo. "Random forests." Machine learning 45 (2001): 5-32. **Random Forests**
* Bengio, Yoshua, Aaron Courville, and Pascal Vincent. "Representation learning: A review and new perspectives." IEEE transactions on pattern analysis and machine intelligence 35.8 (2013): 1798-1828. **Representation Learning**
* He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. **ResNet**
* He, Kaiming, et al. "Mask r-cnn." Proceedings of the IEEE international conference on computer vision. 2017. **Mask R-CNN**
* Carion, Nicolas, et al. "End-to-end object detection with transformers." European conference on computer vision. Cham: Springer International Publishing, 2020.
* Mildenhall, Ben, et al. "Nerf: Representing scenes as neural radiance fields for view synthesis." Communications of the ACM 65.1 (2021): 99-106. **Nerf**
* Touvron, Hugo, et al. "Training data-efficient image transformers & distillation through attention." International conference on machine learning. PMLR, 2021. **Data-efficient image transformers (DeiT)**
* Dosovitskiy, Alexey, et al. "An image is worth 16x16 words: Transformers for image recognition at scale." arXiv preprint arXiv:2010.11929 (2020). **ViT**
* Liu, Ze, et al. "Swin transformer: Hierarchical vision transformer using shifted windows." Proceedings of the IEEE/CVF international conference on computer vision. 2021. **SWIN**
* Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. **Latent Diffusion Models**
* Bao, Fan, et al. "All are worth words: A vit backbone for diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023. **U-ViT**
* Radford, Alec, et al. "Learning transferable visual models from natural language supervision." International conference on machine learning. PMLR, 2021. **CLIP**
* Yu, Jiahui, et al. "Coca: Contrastive captioners are image-text foundation models." arXiv preprint arXiv:2205.01917 (2022). **CoCa**
* Li, Boyi, et al. "Language-driven semantic segmentation." arXiv preprint arXiv:2201.03546 (2022).
* Gu, Albert, and Tri Dao. "Mamba: Linear-time sequence modeling with selective state spaces." arXiv preprint arXiv:2312.00752 (2023). **MAMBA**
* Qu, Haohao, et al. "A survey of mamba." arXiv preprint arXiv:2408.01129 (2024).
* Shumailov, Ilia, et al. "AI models collapse when trained on recursively generated data." Nature 631.8022 (2024): 755-759.
* He, Kaiming, et al. "Masked autoencoders are scalable vision learners." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. **MAE**
* Huang, Po-Yao, et al. "Masked autoencoders that listen." Advances in Neural Information Processing Systems 35 (2022): 28708-28720. **Audio MAE**
* Gupta, Agrim, et al. "Siamese masked autoencoders." Advances in Neural Information Processing Systems 36 (2023): 40676-40693. **SiamMAE**
* Bachmann, Roman, et al. "Multimae: Multi-modal multi-task masked autoencoders." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. **MultiMAE**


